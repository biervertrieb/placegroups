{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "providing ../data/raw/2022_place_canvas_history-000000000003.csv ...\n",
      "not found. need to download ../data/raw/2022_place_canvas_history-000000000003.csv.gzip ...\n",
      "downloading from https://placedata.reddit.com/data/canvas-history/2022_place_canvas_history-000000000003.csv.gzip to ../data/raw/2022_place_canvas_history-000000000003.csv.gzip\n",
      "unpacking ../data/raw/2022_place_canvas_history-000000000003.csv.gzip into ../data/raw/2022_place_canvas_history-000000000003.csv\n",
      "deleting ../data/raw/2022_place_canvas_history-000000000003.csv.gzip\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vscode/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vscode/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-86a2e06f-c8e4-46a6-8461-9e0a363ca9c9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 71ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-86a2e06f-c8e4-46a6-8461-9e0a363ca9c9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "22/07/05 18:55:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+---+----+---+-----------+\n",
      "|             user_id| x1|  y1| x2|  y2|  t|pixel_color|\n",
      "+--------------------+---+----+---+----+---+-----------+\n",
      "|m8NEcPbf5XRV5ppeu...|298|1805|329|1839| 95|    #FFB470|\n",
      "|LKS2u3QL2N3Olv7rn...|257|1736|296|1780|321|    #FFB470|\n",
      "|q/Dk6lmcXm8bcDbNI...|298|1770|334|1803|  0|    #FFB470|\n",
      "|HkR0yRQUJ1wsjh4Zo...|251|1805|296|1812|427|    #FFF8B8|\n",
      "|7JiQyrONpFJphvBEP...|271|1835|296|1859|562|    #FFF8B8|\n",
      "+--------------------+---+----+---+----+---+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from src.data.dataset_functions import get_dataframei_onlymods\n",
    "\n",
    "modframe = get_dataframei_onlymods(3)\n",
    "\n",
    "modframe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "providing ../data/raw/2022_place_canvas_history-000000000003.csv ...\n",
      "../data/raw/2022_place_canvas_history-000000000003.csv is already in data/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from src.data.dataset_functions import get_dataframei\n",
    "\n",
    "dataFrame = get_dataframei(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from src.features.feature_functions import get_latestpixels_from_box\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import functools\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName('Empty_Dataframe').getOrCreate()\n",
    "\n",
    "#Mergen zweier DataFrames\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)\n",
    "\n",
    "#Ja, datacollect ist ineffizient ABER es gibt nicht so viele Zensuren, daher sollte das halb so wild sein!\n",
    "mods = modframe.drop('pixel_color').drop('user_id').collect()\n",
    "\n",
    "emp_RDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Create an expected schema\n",
    "columns = StructType([StructField('x',\n",
    "                                  IntegerType(), True),\n",
    "                    StructField('y',\n",
    "                                IntegerType(), True),\n",
    "                    StructField('user_id',\n",
    "                                StringType(), True),\n",
    "                    StructField('t',\n",
    "                                LongType(), True),\n",
    "                    StructField('pixel_color',\n",
    "                                StringType(), True)])\n",
    "\n",
    "\n",
    "censoredData = spark.createDataFrame(data = emp_RDD,\n",
    "                                    schema = columns)\n",
    "\n",
    "#DataFrame für alle gemachten Zensuren mit den betroffenen Pixel generieren\n",
    "for row in mods:\n",
    "    min_x,min_y,max_x,max_y,tz = list(row)\n",
    "    censoredData = unionAll([censoredData,get_latestpixels_from_box(dataFrame,min_x,min_y,max_x,max_y,tz)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+---+-----------+\n",
      "|  x|   y|             user_id|  t|pixel_color|\n",
      "+---+----+--------------------+---+-----------+\n",
      "|299|1812|QH6feXGSBIZObcDe/...| 73|    #FFB470|\n",
      "|300|1805|RFAixfEVBuu5IBfU2...|  6|    #FFFFFF|\n",
      "|300|1806|q5z+vy8G2Xu6bvO4P...|  7|    #00A368|\n",
      "|300|1807|d4rv9KUGfHHVSo+Kp...| 36|    #FF99AA|\n",
      "|301|1807|+3U9zAP3LWm2kMnJP...| 50|    #FF99AA|\n",
      "+---+----+--------------------+---+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "372"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "censoredData.show(5)\n",
    "censoredData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4e31ce\">\n",
    "\n",
    "So bis hier wurde die Funktion nun für den GESAMTEN Datensatz verallgemeinert....\n",
    "\n",
    "Jetzt muss DataFrame mit user_ids für Bots generiert werden und dann anhand eines Joins geschaut werden, welche Pixel zu Bots gehören.\n",
    "\n",
    "Dann kann count() für das DataFrame mit den Bots angewendet werden und count() für die Ausgangstabelle mit den von der Zensur betroffenen\n",
    "Pixeln, sodass das eine durch das andere geteilt werden kann um entsprechend das Verhältnis rauszubekommen!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
