{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "providing ../data/raw/2022_place_canvas_history-000000000003.csv ...\n",
      "not found. need to download ../data/raw/2022_place_canvas_history-000000000003.csv.gzip ...\n",
      "downloading from https://placedata.reddit.com/data/canvas-history/2022_place_canvas_history-000000000003.csv.gzip to ../data/raw/2022_place_canvas_history-000000000003.csv.gzip\n",
      "unpacking ../data/raw/2022_place_canvas_history-000000000003.csv.gzip into ../data/raw/2022_place_canvas_history-000000000003.csv\n",
      "deleting ../data/raw/2022_place_canvas_history-000000000003.csv.gzip\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vscode/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vscode/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4b8e5ec7-07a4-4d13-9c76-c08c366c60bd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 72ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4b8e5ec7-07a4-4d13-9c76-c08c366c60bd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "22/07/08 11:05:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Stage 2:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+---+----+------+-----------+\n",
      "|             user_id| x1|  y1| x2|  y2|     t|pixel_color|\n",
      "+--------------------+---+----+---+----+------+-----------+\n",
      "|m8NEcPbf5XRV5ppeu...|298|1805|329|1839|216904|    #FFB470|\n",
      "|LKS2u3QL2N3Olv7rn...|257|1736|296|1780|217130|    #FFB470|\n",
      "|q/Dk6lmcXm8bcDbNI...|298|1770|334|1803|216809|    #FFB470|\n",
      "|HkR0yRQUJ1wsjh4Zo...|251|1805|296|1812|217236|    #FFF8B8|\n",
      "|7JiQyrONpFJphvBEP...|271|1835|296|1859|217371|    #FFF8B8|\n",
      "+--------------------+---+----+---+----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from src.data.dataset_functions import get_dataframe_onlymods_full\n",
    "\n",
    "modframe = get_dataframe_onlymods_full(True)\n",
    "#modframe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "providing ../data/raw/2022_place_canvas_history-000000000003.csv ...\n",
      "../data/raw/2022_place_canvas_history-000000000003.csv is already in data/raw\n"
     ]
    }
   ],
   "source": [
    "from src.data.dataset_functions import get_dataframe\n",
    "\n",
    "dataFrame = get_dataframe(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from src.features.feature_functions import get_latestpixels_from_box\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import functools\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName('Empty_Dataframe').getOrCreate()\n",
    "\n",
    "#Mergen zweier DataFrames\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)\n",
    "\n",
    "#Ja, datacollect ist ineffizient ABER es gibt nicht so viele Zensuren, daher sollte das halb so wild sein!\n",
    "mods = modframe.drop('pixel_color').drop('user_id').collect()\n",
    "\n",
    "emp_RDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Create an expected schema\n",
    "columns = StructType([StructField('x',\n",
    "                                IntegerType(), True),\n",
    "                    StructField('y',\n",
    "                                IntegerType(), True),\n",
    "                    StructField('user_id',\n",
    "                                StringType(), True),\n",
    "                    StructField('t',\n",
    "                                LongType(), True),\n",
    "                    StructField('pixel_color',\n",
    "                                StringType(), True)])\n",
    "\n",
    "\n",
    "censoredData = spark.createDataFrame(data = emp_RDD,\n",
    "                                    schema = columns)\n",
    "\n",
    "#DataFrame für alle gemachten Zensuren mit den betroffenen Pixel generieren\n",
    "for row in mods:\n",
    "    min_x,min_y,max_x,max_y,tz = list(row)\n",
    "    censoredData = unionAll([censoredData,get_latestpixels_from_box(dataFrame,min_x,min_y,max_x,max_y,tz)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+------+-----------+\n",
      "|  x|   y|             user_id|     t|pixel_color|\n",
      "+---+----+--------------------+------+-----------+\n",
      "|298|1805|8Ncd5vj/7mJHwq1z6...|216819|    #FFB470|\n",
      "|298|1809|Wf5SrgaWTWXW4Yn5Q...|216828|    #FF99AA|\n",
      "|298|1811|ZYSDFfRaU6mr2/GHW...|216682|    #FFB470|\n",
      "|298|1812|dd7OsqTULsU0SFGsv...|216853|    #FFB470|\n",
      "|298|1834|93RvWf9B2YUC7tubU...|216691|    #000000|\n",
      "+---+----+--------------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#censoredData.show(5)\n",
    "total_censors = censoredData.count() #Anzahl der betroffenen Pixel speichern (später für Statistik wichtig!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4e31ce\">\n",
    "\n",
    "So bis hier wurde die Funktion nun für den GESAMTEN Datensatz verallgemeinert....\n",
    "\n",
    "Jetzt muss DataFrame mit user_ids für Bots generiert werden und dann anhand eines Joins geschaut werden, welche Pixel zu Bots gehören.\n",
    "\n",
    "Dann kann count() für das DataFrame mit den Bots angewendet werden und count() für die Ausgangstabelle mit den von der Zensur betroffenen\n",
    "Pixeln, sodass das eine durch das andere geteilt werden kann um entsprechend das Verhältnis rauszubekommen!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4e31ce\">\n",
    "Dann beginnen wir mal das DataFrame mit den user_ids für die Bots zu generieren.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "botDiff = timeDiffReduced.alias('tdr1').join(timeDiffReduced.alias('tdr2'),F.col('tdr1.user_id') == F.col('tdr2.user_id'),'inner').join(timeDiffReduced.alias('tdr3'),F.col('tdr1.user_id') == F.col('tdr3.user_id'), 'inner')\n",
    "\n",
    "\n",
    "botDiff = botDiff.where('tdr1.diff == tdr2.diff').where('tdr1.t2 == tdr2.t1').where('tdr3.diff == tdr1.diff').where('tdr2.t2 == tdr3.t1')\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame ist schon vorhanden, siehe oben!\n",
    "timeDiffFrame = dataFrame.alias('df1').join(dataFrame.alias('df2'),F.col('df1.user_id') == F.col('df2.user_id'),'inner')\n",
    "timeDiffFrame = timeDiffFrame.where('df2.t > df1.t')\n",
    "timeDiffFrame = timeDiffFrame.orderBy(['df1.user_id', 'df1.t'])\n",
    "timeDiffFrame = timeDiffFrame.select(F.col('df1.user_id'),F.col('df1.t').alias('t1'),F.col('df2.t').alias('t2'),(F.col('df2.t') - F.col('df1.t')).alias('diff'))\n",
    "#timeDiffFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDiffReduced = timeDiffFrame.dropDuplicates(['user_id', 't1'])\n",
    "timeDiffReduced = timeDiffReduced.orderBy(['user_id', 't1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- diff: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "botDiff = timeDiffReduced.alias('tdr1').join(timeDiffReduced.alias('tdr2'),F.col('tdr1.user_id') == F.col('tdr2.user_id'),'inner').join(timeDiffReduced.alias('tdr3'),F.col('tdr1.user_id') == F.col('tdr3.user_id'), 'inner')\n",
    "botDiff = botDiff.where('tdr1.diff == tdr2.diff').where('tdr1.t2 == tdr2.t1').where('tdr3.diff == tdr1.diff').where('tdr2.t2 == tdr3.t1')\n",
    "botDiff = botDiff.select('tdr1.user_id','tdr1.diff')\n",
    "botDiff = botDiff.dropDuplicates(['user_id'])\n",
    "botDiff = botDiff.orderBy(['user_id'])\n",
    "#botDiff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "botDiff = botDiff.drop(\"diff\")\n",
    "#botDiff.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4e31ce\">\n",
    "Das DataFrame botDiff ist dann unsere Liste an Bots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4e31ce\">\n",
    "Jetzt müssen die beiden DataFrames censoredData und botDiff gejoined werden übder die user_ids, sodass nur noch Bots erhalten bleiben.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: long (nullable = true)\n",
      " |-- y: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- t: long (nullable = true)\n",
      " |-- pixel_color: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedCensors = censoredData.alias('cd').join(botDiff.alias('bd'),F.col('cd.user_id') == F.col('bd.user_id'),'inner')\n",
    "#joinedCensors.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/08 11:11:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/08 11:11:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/08 11:11:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/08 11:11:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---+-----------+-------+\n",
      "|  x|  y|user_id|  t|pixel_color|user_id|\n",
      "+---+---+-------+---+-----------+-------+\n",
      "+---+---+-------+---+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedCensors = joinedCensors.drop('bd.user_id')\n",
    "#joinedCensors.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/08 11:18:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/08 11:18:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/08 11:18:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/08 11:18:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Von den zensierten Pixeln wurden  0.0 % von Bots und  100.0 % von Nutzern gesetzte Pixel zensiert.\n"
     ]
    }
   ],
   "source": [
    "#Jetzt nur noch Menge zählen für joinedCensors\n",
    "bots = joinedCensors.count()\n",
    "#falls Bots zensiert wurden, muss bots != 0 sein\n",
    "if(bots is not None or bots == 0):\n",
    "    censoredBots = bots / total_censors\n",
    "    censoredUsers = 100 - censoredBots\n",
    "#wurden keine Bots zensiert (sehr unwahrscheinlich) ist bots = 0\n",
    "else: \n",
    "    censoredBots = 0\n",
    "    censoredUsers = 100\n",
    "print(\"Von den zensierten Pixeln wurden \",censoredBots,\"% von Bots und \",censoredUsers,\"% von Nutzern gesetzte Pixel zensiert.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
